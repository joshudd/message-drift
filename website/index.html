<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>
      NLP Class Project | Spring 2025 CSCI 5541 | University of Minnesota
    </title>

    <link rel="stylesheet" href="./src/bulma.min.css" />

    <link rel="stylesheet" href="./src/styles.css" />
    <link rel="preconnect" href="https://fonts.gstatic.com/" />
    <link href="./src/css2.txt" rel="stylesheet" />
    <link href="./src/css.txt" rel="stylesheet" />

    <base href="." target="_blank" />
  </head>

  <body>
    <div>
      <div class="wrapper">
        <h1 style="font-family: 'Lato', sans-serif">
          A Study on Information Transmission and Transformation in LLM-Based
          Multi-Agent Systems for Effective Collaboration
        </h1>
        <h4 style="font-family: 'Lato', sans-serif">
          Spring 2025 CSCI 5541 NLP: Class Project - University of Minnesota
        </h4>
        <h4 style="font-family: 'Lato', sans-serif">
          InterAgent Communication Lab
        </h4>

        <div class="authors-wrapper">
          <div class="author-container">
            <!-- <div class="author-image">
              <img src="" />
            </div> -->
            <p>Be√±at Froemming-Aldanondo</p>
          </div>

          <div class="author-container">
            <!-- <div class="author-image">
              <img src="" />
            </div> -->
            <p>Joshua Dickinson</p>
          </div>

          <div class="author-container">
            <!-- <div class="author-image">
              <img src="" />
            </div> -->
            <p>Daniel Bielejeski</p>
          </div>

          <div class="author-container">
            <!-- <div class="author-image">
              <img src="" />
            </div> -->
            <p>Isaac Ash-Johnson</p>
          </div>
        </div>

        <br />

        <div class="authors-wrapper">
          <div class="publication-links">
            <!-- <span class="link-block">
              <a
                href=""
                target="_blank"
                class="external-link button is-normal is-rounded is-dark is-outlined"
              >
                <span>Final Report</span>
              </a>
            </span> -->
            <span class="link-block">
              <a
                href="https://github.com/joshudd/message-drift"
                target="_blank"
                class="external-link button is-normal is-rounded is-dark is-outlined"
              >
                <span>GitHub</span>
              </a>
            </span>
          </div>
        </div>
      </div>
    </div>

    <div class="wrapper">
      <hr />

      <h2 id="abstract">Abstract</h2>

      <p>
        While significant research has focused on the behavior of individual large language models (LLMs), less attention has been paid to how information is transmitted and transformed between them. In human communication, it is well known that stories evolve as they are passed down through speech and writing. Inspired by this phenomenon, we investigate whether LLMs preserve message integrity in a simulated "telephone game" setting. Our study adopts a multi-agent framework motivated by swarm robotics, where distributed agents must share tasks without losing essential information. Experiments using GPT-4.1 reveal that message degradation occurs in distinct ways depending on prompt design.
      </p>

      <hr />

      <h2 id="teaser">Preliminary Results</h2>

      <p>
        Our experiments with different prompting strategies reveal how messages change as they propagate through a network of LLMs. Below are visualizations showing how various metrics degrade through message propagation.
      </p>

      <div style="text-align: center">
        <p class="sys-img">
          <img
            src="./assets/rouge-tree.png"
            alt="Agent tree with ROUGE scores showing information degradation"
          />
        </p>
        <caption>
          Figure 5. ROUGE-1 scores across agent generations
        </caption>
      </div>

      <br />
      <br />

      <div style="text-align: center">
        <p class="sys-img">
          <img
            src="./assets/similarity-tree.png"
            alt="Agent tree with cosine similarity scores showing information degradation"
          />
        </p>
        <caption>
          Figure 6. Cosine similarity scores across agent generations
        </caption>
      </div>

      <h3 id="the-timeline-and-the-highlights">Message Transformation</h3>

      <p>
        Our experiments reveal that even simple prompt variations can lead to significant semantic drift, affecting both message clarity and task success. We tested multiple prompting strategies including copy-paste, rewording, explaining, summarizing, changing tone, optimizing, expanding, distorting, explaining to a child, and breaking into steps. While some strategies preserved meaning better than others, no method was immune to degradation over multiple generations besides direct copy-paste.
      </p>

      <hr />

      <h2 id="introduction">Introduction / Background / Motivation</h2>

      <p>
        <b
          >What did you try to do? What problem did you try to solve? Articulate
          your objectives using absolutely no jargon.</b
        >
      </p>
      <p>
        Throughout history, human communication demonstrates that information
        rarely remains unchanged as it passes from person to person. Stories,
        legends, and historical events evolve over time, shaped by biases,
        interpretations, and conflicting memories of those who transmit them.
        The legend of King Arthur illustrates this phenomenon, as it has been
        told and adapted by many authors throughout centuries. We investigate
        whether large language models (LLMs) exhibit similar behaviors in
        information transmission. Using a simulated "telephone game" experiment
        with multiple successors rather than linear chains, we aim to understand
        how messages transform as they propagate through a network of AI agents,
        similar to information spread in human populations.
      </p>

      <p>
        <b
          >How is it done today, and what are the limits of current practice?</b
        >
      </p>
      <p>
        While extensive research has explored the behavior of individual LLMs,
        studies on their interaction are relatively limited. Existing research
        on the telephone game with LLMs primarily focuses on linear chains of
        transmission. For example, one study by Perez et al. (2024) tracked how
        text properties like toxicity, positivity, difficulty, and length evolve
        through iterated interactions between LLMs. Another multilingual study
        by Mohamed et al. (2025) demonstrated that distortion increases with
        time and complexity but can be mitigated with strategic prompting.
        However, these studies are limited to linear chains, which don't
        accurately reflect how information spreads in real-world populations or
        collaborative multi-agent systems.
      </p>

      <p></p>

      <p>
        <b>Who cares? If you are successful, what difference will it make?</b>
      </p>
      <p>
        Understanding information transmission between LLMs has critical
        implications for collaborative AI systems, particularly in applications
        like swarm robotics. Imagine a swarm of robots collaborating on a common
        task, where only one robot receives the initial instructions. If each
        robot has its own LLM, the message must propagate accurately through the
        swarm. Any distortion could cause robots to pursue different objectives,
        compromising collaborative efficiency. Our research not only enhances
        understanding of LLMs' potential for cumulative cultural evolution and
        their vulnerability to biases but also provides key insights for
        designing reliable AI communication systems where message integrity is
        essential for coordinated action. The findings could inform better agent
        communication protocols, more robust information sharing mechanisms, and
        improved task delegation in complex AI systems.
      </p>

      <hr />

      <h2 id="approach">Approach</h2>

      <p>
        <b
          >What did you do exactly? How did you solve the problem? Why did you
          think it would be successful? Is anything new in your approach?</b
        >
      </p>

      <p>
        Our research introduces two key innovations in studying LLM information transmission:
      </p>
      
      <ol>
        <li>
          We extend beyond traditional linear chain "telephone game" studies by incorporating multiple successors and one-to-many relationships, better simulating message propagation across a population or swarm of agents.
        </li>
        <li>
          Unlike prior studies that only focus on rewording or paraphrasing in each propagation step, we test 10 different propagation strategies including direct, explanatory, and adversarial prompts.
        </li>
      </ol>
      
      <p>
        We implemented a pipeline that begins by randomly selecting tasks from datasets like HowTo100M (YouTube tutorial transcripts) and RecipeBox (recipe instructions). We then initialize a random communication tree with parameters including depth=8, num_agents=80, and max_children=3, creating a structure where each node has a single parent and 0-3 children.
      </p>
      
      <p>
        Starting from the root node, the task propagates through the tree using different prompting strategies: copy-paste, reword, explain, summarize, change tone to casual, optimize, expand, distort, explain to a child, and break into steps. Each node's output becomes the input for its children.
      </p>

      <div style="text-align: center">
        <p class="sys-img">
          <img
            src="./assets/pipeline.png"
            alt="Pipeline diagram showing the experiment process"
          />
        </p>
        <caption>
          Figure 1. Pipeline for the telephone game experiment
        </caption>
      </div>

      <p>To measure information degradation, we applied three key metrics:</p>
      <ol>
        <li>
          Length ratio: comparing how message length changes through propagation
        </li>
        <li>
          ROUGE score: comparing word overlap between original and propagated messages
        </li>
        <li>
          Cosine similarity of embeddings: measuring semantic drift despite different wording
        </li>
      </ol>

      <p>
        <b
          >What problems did you anticipate? What problems did you encounter?
          Did the very first thing you tried work?</b
        >
      </p>

      <p>
        We encountered significant challenges when testing with smaller models like LLaMA 3.2-3B and Mistral 7B Instruct. Both showed rapid degradation primarily due to formatting and prompt interpretation issues. Since each model's output became the input for the next agent, consistent formatting was critical. In contrast, GPT-4.1 performed reliably with minimal guidance.
      </p>
      
      <p>
        In our second experiment involving problem-solving tasks, we found that explanatory prompts often omitted critical information such as numerical values, rendering many problems unsolvable. Overall, prompt design and formatting had a significant impact on message transmission reliability.
      </p>

      <hr />

      <h2 id="results">Results</h2>
      <p>
        <b
          >How did you measure success? What experiments were used? What were
          the results, both quantitative and qualitative? Did you succeed? Did
          you fail? Why?</b
        >
      </p>
      
      <p>
        We conducted experiments on five distinct randomly generated tasks and tree structures, tracking how messages changed through propagation. Our findings show:
      </p>
      
      <div style="text-align: center">
        <p class="sys-img">
          <img
            src="./assets/chart0.png"
            alt="Chart showing average length ratio by depth"
          />
        </p>
        <caption>
          Figure 2. Average Length Ratio by depth
        </caption>
      </div>

      <div style="text-align: center">
        <p class="sys-img">
          <img
            src="./assets/chart1.png"
            alt="Chart showing average ROUGE score by depth"
          />
        </p>
        <caption>
          Figure 3. Average ROUGE score by depth
        </caption>
      </div>

      <div style="text-align: center">
        <p class="sys-img">
          <img
            src="./assets/chart2.png"
            alt="Chart showing average embeddings similarity by depth"
          />
        </p>
        <caption>
          Figure 4. Average embeddings similarity by depth
        </caption>
      </div>
      
      <ol>
        <li>
          <strong>Message Length:</strong> Copy-paste preserved original length as expected. Rewording, breaking into steps, and child-friendly explanations slightly altered length. Expanding tasks significantly increased length, while distortion and explanation caused moderate growth. Casual tone conversion, summarization, and optimization consistently shortened messages.
        </li>
        <li>
          <strong>Word Overlap (ROUGE):</strong> Shared word sequences dropped sharply after the first transmission and then stabilized. Distortion and expansion prompts altered wording most significantly.
        </li>
        <li>
          <strong>Semantic Preservation:</strong> Rewording retained meaning most closely, aligning with prior research. However, summarization and explanation prompts, despite seeming simple, resulted in considerable semantic loss.
        </li>
      </ol>
      
      <p>
        In our problem-solving experiment, we tested whether transformed messages remained functional:
      </p>
      
      <table>
        <thead>
          <tr>
            <th style="text-align: center"><strong>Outcome</strong></th>
            <th style="text-align: center">Reword</th>
            <th style="text-align: center">Explain</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="text-align: center">Correct</td>
            <td style="text-align: center">80</td>
            <td style="text-align: center">38</td>
          </tr>
          <tr>
            <td style="text-align: center">Incorrect</td>
            <td style="text-align: center">0</td>
            <td style="text-align: center">3</td>
          </tr>
          <tr>
            <td style="text-align: center">Not enough info</td>
            <td style="text-align: center">0</td>
            <td style="text-align: center">39</td>
          </tr>
        </tbody>
        <caption>
          Table 1. Outcomes of problem-solving tasks after message propagation
        </caption>
      </table>
      <br />
      
      <p>
        Rewording preserved task accuracy perfectly (80/80 correct), while explaining led to a significant performance drop, with only 38 correct responses, 3 incorrect, and 39 lacking enough information to solve the problem.
      </p>

      <hr />

      <h2 id="conclusion">Conclusion and Future Work</h2>
      <p>
        We investigated how messages change as they propagate through a network of large language models using a branching communication structure and diverse prompting strategies. Our results show that even simple prompt variations can lead to significant semantic drift, affecting both message clarity and task success. Although some strategies preserved meaning better than others, no method was immune to degradation over multiple generations besides direct copy-paste.
      </p>
      
      <p>
        Additionally, smaller models like LLaMA 3.2‚Äì3B and Mistral 7B struggled with formatting and maintaining message integrity, often leading to confusion or hallucination. These findings highlight the difficulty of ensuring reliable communication in multi-agent LLM systems.
      </p>
      
      <p>
        While our work does not pose immediate societal risks, in the future, as swarm robotics becomes more advanced, unreliable communication among LLMs could lead to failures in critical applications. Future directions include exploring dynamic feedback, bidirectional communication, and deploying these methods in physical multi-agent environments, such as robot swarms engaged in collaborative tasks.
      </p>

      <hr />
    </div>
  </body>
</html>
